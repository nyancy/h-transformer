{"cells":[{"cell_type":"markdown","metadata":{"id":"VdTCf3vHc9eQ"},"source":["# Imports and Colab Mount"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WA0xM236cFzP","executionInfo":{"status":"ok","timestamp":1649729519935,"user_tz":-540,"elapsed":4288,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["import datetime\n","import seaborn as sn\n","import pandas as pd\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import scipy.stats as stats\n","import glob\n","from math import sqrt\n","from tqdm import tqdm\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","import keras\n","from tensorflow.keras.optimizers import Adam\n","from keras.layers import Dense, LSTM, LeakyReLU, Dropout, GRU, SimpleRNN, Input, LSTM, Dense, Bidirectional, Concatenate, Reshape, Lambda, Bidirectional\n","from keras.models import Model, Sequential\n","from keras import backend as K\n","from tensorflow.keras import layers\n","from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","\n","from numpy.random import seed\n","#from tensorflow import set_random_seed\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"LIhGU4HNc5uw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649729543414,"user_tz":-540,"elapsed":23491,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}},"outputId":"86944113-1e5b-4c09-c01b-b744ab3eef2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"yz0e7vrtdDWe"},"source":["# Load Data"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NNIJ9EXxdP5f","executionInfo":{"status":"ok","timestamp":1649729547297,"user_tz":-540,"elapsed":3424,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["hourly = pd.read_excel(\"/content/drive/MyDrive/Hybrid Transformer/Datasets/Vento/BASERECIFEhorario.xlsx\", index_col=0, parse_dates=[[\"Data\", \"HORA (UTC)\"]])\n","hourly = hourly[ ['Velocidade'] + [ col for col in hourly.columns if col != 'Velocidade' ] ]\n","quarto = int(hourly[\"Velocidade\"].shape[0]/4)\n","\n","train_h = hourly.iloc[:quarto*2]\n","valid_h = hourly.iloc[quarto*2:quarto*3]\n","test_h = hourly.iloc[quarto*3:]\n","\n","resid_train = pd.read_csv(\"/content/drive/MyDrive/Hybrid Transformer/Datasets/Vento/recifehora_resid_train.csv\").drop(\"Unnamed: 0\", axis=1)\n","resid_test = pd.read_csv(\"/content/drive/MyDrive/Hybrid Transformer/Datasets/Vento/recifemensal_resid_test.csv\").drop(\"Unnamed: 0\", axis=1)\n","\n","sarima_pred = pd.read_csv(\"/content/drive/MyDrive/Hybrid Transformer/Datasets/Vento/recifehora_prediction.csv\").drop(\"Unnamed: 0\", axis=1)\n","sarima_pred2 = pd.read_csv(\"/content/drive/MyDrive/Hybrid Transformer/Datasets/Vento/recifemensal_prediction2.csv\").drop(\"Unnamed: 0\", axis=1)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"twTRqY3aeIX-","executionInfo":{"status":"ok","timestamp":1649729547300,"user_tz":-540,"elapsed":21,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = pd.DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = pd.concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6Ghi6ux7eSHe","executionInfo":{"status":"ok","timestamp":1649729547302,"user_tz":-540,"elapsed":20,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["def make_data(data, timestep, resid_check=False):\n","  values = data.values\n","  values = values.astype('float32')\n","  #scaler = MinMaxScaler(feature_range=(0, 1))\n","  #scaled = scaler.fit_transform(values)\n","  \n","  timestep = timestep\n","  n_features = data.shape[1]\n","  n_obs = timestep * n_features\n","  reframed = series_to_supervised(values, timestep, 1)\n","  reframed = reframed.iloc[: , :1-n_features]\n","\n","  values = reframed.values\n","  indice1 = train_h.shape[0]\n","  indice2 = valid_h.shape[0]\n","\n","  train = values[:indice1, :]\n","  valid = values[indice1:indice1+indice2, :]\n","  test = values[indice1+indice2:, :]\n","\n","  train_X, train_y = train[:, :-1], train[:, -1]\n","  valid_X, valid_y = valid[:, :-1], valid[:, -1]\n","  test_X, test_y = test[:, :-1], test[:, -1]\n","\n","  if (resid_check==True):\n","    train_y = resid_train.values[:quarto*2]\n","    valid_y = resid_train.values[quarto*2:]\n","\n","  scaler = MinMaxScaler(feature_range=(0, 1)).fit(train_X)\n","  train_X = scaler.transform(train_X)\n","  valid_X = scaler.transform(valid_X)\n","  test_X = scaler.transform(test_X)\n","\n","  scaler_y = MinMaxScaler(feature_range=(0, 1)).fit(train_y.reshape(-1,1))\n","  train_y = scaler_y.transform(train_y.reshape(-1,1))\n","  valid_y = scaler_y.transform(valid_y.reshape(-1,1))\n","  test_y = scaler_y.transform(test_y.reshape(-1,1))\n","\n","  train_X = train_X.reshape((train_X.shape[0], timestep, n_features))\n","  valid_X = valid_X.reshape((valid_X.shape[0], timestep, n_features))\n","  test_X = test_X.reshape((test_X.shape[0], timestep, n_features))\n","  return train_X, train_y, valid_X, valid_y, test_X, test_y"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZGNGDxk7f3lP","executionInfo":{"status":"ok","timestamp":1649729547729,"user_tz":-540,"elapsed":9,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["def make_data2(data, timestep, resid_check=False):\n","  values = data.values\n","  values = values.astype('float32')\n","  #scaler = MinMaxScaler(feature_range=(0, 1))\n","  #scaled = scaler.fit_transform(values)\n","  \n","  timestep = timestep\n","  n_features = data.shape[1]\n","  n_obs = timestep * n_features\n","  reframed = series_to_supervised(values, timestep, 1)\n","  reframed = reframed.iloc[: , :1-n_features]\n","\n","  values = reframed.values\n","  indice1 = train_h.shape[0]\n","  indice2 = valid_h.shape[0]\n","\n","  train = values[:indice1, :]\n","  valid = values[indice1:indice1+indice2, :]\n","  test = values[indice1+indice2:, :]\n","\n","  train_X, train_y = train[:, :-1], train[:, -1]\n","  valid_X, valid_y = valid[:, :-1], valid[:, -1]\n","  test_X, test_y = test[:, :-1], test[:, -1]\n","\n","  if (resid_check==True):\n","    train_y = resid_train.values[:quarto*2]\n","    valid_y = resid_train.values[quarto*2:]\n","\n","  scaler = MinMaxScaler(feature_range=(0, 1)).fit(train_X)\n","  train_X = scaler.transform(train_X)\n","  valid_X = scaler.transform(valid_X)\n","  test_X = scaler.transform(test_X)\n","\n","  scaler_y = MinMaxScaler(feature_range=(0, 1)).fit(train_y.reshape(-1,1))\n","  train_y = scaler_y.transform(train_y.reshape(-1,1))\n","  valid_y = scaler_y.transform(valid_y.reshape(-1,1))\n","  test_y = scaler_y.transform(test_y.reshape(-1,1))\n","\n","  train_X = train_X.reshape((train_X.shape[0], timestep, n_features))\n","  valid_X = valid_X.reshape((valid_X.shape[0], timestep, n_features))\n","  test_X = test_X.reshape((test_X.shape[0], timestep, n_features))\n","  return train_X, train_y, valid_X, valid_y, test_X, test_y, scaler, scaler_y"]},{"cell_type":"markdown","metadata":{"id":"Wckz4IzLgVFo"},"source":["# Test"]},{"cell_type":"markdown","metadata":{"id":"15vG2PmdmOKl"},"source":["# RNN"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qIM46d7mmspt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649729590120,"user_tz":-540,"elapsed":41765,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}},"outputId":"5226cd2c-4504-4c5d-f833-22541bfb7a48"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|█         | 1/10 [00:04<00:42,  4.71s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 2/10 [00:08<00:33,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 3/10 [00:14<00:35,  5.14s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 4/10 [00:18<00:26,  4.46s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 5/10 [00:21<00:20,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 6/10 [00:27<00:19,  4.80s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 7/10 [00:31<00:13,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 8/10 [00:34<00:08,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 9/10 [00:38<00:03,  3.97s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:41<00:00,  4.15s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","RNN LIST OF RMSE: [0.7003577135803072, 0.6865878122461965, 0.7350159952148938, 0.7259039406608112, 0.6840407081991221, 0.6810548117757584, 0.7021280850788705, 0.7446157785728053, 0.7084407350982045, 0.7416116990568848]\n","RNN RMSE:  0.7109757279483854\n","RNN LIST OF MAE: [0.5439818, 0.52814007, 0.55447, 0.5702471, 0.51324546, 0.51443946, 0.5374672, 0.56139594, 0.53814703, 0.5699665]\n","RNN MAE:  0.5431500554084778\n","RNN LIST OF MAPE: [0.19444776, 0.194544, 0.23165487, 0.2028542, 0.20964135, 0.19123039, 0.19582798, 0.20988613, 0.19791694, 0.20368053]\n","RNN MAPE:  0.20316841453313828\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rmse_list = []\n","mae_list = []\n","mape_list = []\n","\n","timestep=5\n","layers=1\n","num_units=128\n","dropout=0.1\n","lr=0.01\n","batch_size=32\n","\n","for z in tqdm(range(10)):\n","  train_X, train_y, valid_X, valid_y, test_X, test_y, scaler, scaler_y = make_data2(hourly, 6, resid_check=False)\n","  model = Sequential()\n","\n","  if layers > 1:\n","    model.add(SimpleRNN(units = num_units, input_shape=(train_X.shape[1], train_X.shape[2]), dropout=dropout, return_sequences=True))\n","\n","    for i in range(layers-2):\n","      model.add(SimpleRNN(units = num_units, dropout=dropout, return_sequences=True))\n","\n","    model.add(SimpleRNN(units = num_units, dropout=dropout))\n","\n","  else:\n","    model.add(SimpleRNN(units = num_units, input_shape=(train_X.shape[1], train_X.shape[2]), dropout=dropout))\n","\n","  model.add(Dense(units = 1))\n","\n","  model.compile(\n","    loss=\"mse\",\n","    optimizer=Adam(learning_rate=lr)\n","  )\n","\n","  model.fit(train_X, train_y, batch_size=batch_size,\n","          epochs=200, verbose=0, shuffle=False,\n","          validation_data=(valid_X, valid_y),\n","          callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n","  \n","  # make a prediction\n","  yhat = model.predict(test_X)\n","  yhat_inv = scaler_y.inverse_transform(yhat)\n","  #resid_sum = (yhat_inv+resid_test.values[3:])\n","\n","  rmse_list.append(sqrt(mean_squared_error(yhat_inv, scaler_y.inverse_transform(test_y))))\n","  mae_list.append(mean_absolute_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","  mape_list.append(mean_absolute_percentage_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","  print()\n","print(f\"RNN LIST OF RMSE: {rmse_list}\")\n","print(f'RNN RMSE:  {sum(rmse_list)/len(rmse_list)}')\n","\n","print(f\"RNN LIST OF MAE: {mae_list}\")\n","print(f'RNN MAE:  {sum(mae_list)/len(mae_list)}')\n","\n","print(f\"RNN LIST OF MAPE: {mape_list}\")\n","print(f'RNN MAPE:  {sum(mape_list)/len(mape_list)}')"]},{"cell_type":"markdown","metadata":{"id":"Op-rtqjIhCZO"},"source":["# LSTM"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"uutPNh2svbbm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649729660666,"user_tz":-540,"elapsed":70560,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}},"outputId":"ba375392-78ea-43b4-a0bd-36e8d6149e45"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|█         | 1/10 [00:08<01:14,  8.32s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 2/10 [00:13<00:52,  6.51s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 3/10 [00:18<00:41,  5.95s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 4/10 [00:26<00:40,  6.81s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 5/10 [00:34<00:36,  7.24s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 6/10 [00:42<00:29,  7.38s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 7/10 [00:50<00:22,  7.65s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 8/10 [00:58<00:15,  7.67s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 9/10 [01:03<00:06,  6.98s/it]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [01:10<00:00,  7.08s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","LSTM LIST OF RMSE: [0.7605595302067697, 0.7454316961593923, 0.7346152054797602, 0.7348605235101909, 0.6314389191624487, 0.7628014328281526, 0.6536766414803654, 0.7354865893730375, 0.7406907096717927, 0.649762695349927]\n","LSTM RMSE:  0.7149323943221837\n","LSTM LIST OF MAE: [0.5945618, 0.5815748, 0.5750044, 0.5737687, 0.46701297, 0.5940406, 0.4842424, 0.57369465, 0.5762544, 0.48351163]\n","LSTM MAE:  0.5503666341304779\n","LSTM LIST OF MAPE: [0.22475283, 0.21925092, 0.21008869, 0.21311879, 0.17902598, 0.22112305, 0.20590444, 0.21521372, 0.21732926, 0.19995244]\n","LSTM MAPE:  0.21057601124048234\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rmse_list = []\n","mae_list = []\n","mape_list = []\n","\n","timestep=3\n","layers=1\n","num_units=256\n","dropout=0\n","lr=0.01\n","batch_size=64\n","\n","for z in tqdm(range(10)):\n","  train_X, train_y, valid_X, valid_y, test_X, test_y, scaler, scaler_y = make_data2(hourly, timestep, resid_check=False)\n","  model = Sequential()\n","\n","  if layers > 1:\n","    model.add(LSTM(units = num_units, input_shape=(train_X.shape[1], train_X.shape[2]), dropout=dropout, return_sequences=True))\n","\n","    for i in range(layers-2):\n","      model.add(LSTM(units = num_units, dropout=dropout, return_sequences=True))\n","\n","    model.add(LSTM(units = num_units, dropout=dropout))\n","\n","  else:\n","    model.add(LSTM(units = num_units, input_shape=(train_X.shape[1], train_X.shape[2]), dropout=dropout))\n","\n","  model.add(Dense(units = 1))\n","\n","  model.compile(\n","    loss=\"mse\",\n","    optimizer=Adam(learning_rate=lr)\n","  )\n","\n","  model.fit(train_X, train_y, batch_size=batch_size,\n","          epochs=200, verbose=0, shuffle=False,\n","          validation_data=(valid_X, valid_y),\n","          callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n","  \n","  # make a prediction\n","  yhat = model.predict(test_X)\n","  yhat_inv = scaler_y.inverse_transform(yhat)\n","  #resid_sum = (yhat_inv+resid_test.values[3:])\n","\n","  rmse_list.append(sqrt(mean_squared_error(yhat_inv, scaler_y.inverse_transform(test_y))))\n","  mae_list.append(mean_absolute_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","  mape_list.append(mean_absolute_percentage_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","  print()\n","print(f\"LSTM LIST OF RMSE: {rmse_list}\")\n","print(f'LSTM RMSE:  {sum(rmse_list)/len(rmse_list)}')\n","\n","print(f\"LSTM LIST OF MAE: {mae_list}\")\n","print(f'LSTM MAE:  {sum(mae_list)/len(mae_list)}')\n","\n","print(f\"LSTM LIST OF MAPE: {mape_list}\")\n","print(f'LSTM MAPE:  {sum(mape_list)/len(mape_list)}')"]},{"cell_type":"markdown","metadata":{"id":"nZYNeaW_mD9c"},"source":["# GRU"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"S3k5K35Dvj2b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649729831252,"user_tz":-540,"elapsed":72828,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}},"outputId":"6f08a99c-f6a8-48af-8f3e-9921b54a4afe"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [01:12<00:00,  7.27s/it]"]},{"output_type":"stream","name":"stdout","text":["GRU LIST OF RMSE: [0.6538790835043569, 0.7020844070886689, 0.6890127272157085, 0.6889244408389368, 0.754932318047425, 0.652009044372396, 0.6806495697799149, 0.6870141696953993, 0.6384025962650716, 0.65361408647949]\n","GRU RMSE:  0.6800522443287368\n","GRU LIST OF MAE: [0.49601683, 0.52091444, 0.5163103, 0.51691467, 0.564438, 0.49293265, 0.5187793, 0.51735765, 0.4794026, 0.494641]\n","GRU MAE:  0.511770737171173\n","GRU LIST OF MAPE: [0.18771555, 0.21403438, 0.20255671, 0.20246017, 0.23699723, 0.18767487, 0.19576979, 0.20168878, 0.18109241, 0.18782921]\n","GRU MAPE:  0.19978190958499908\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rmse_list = []\n","mae_list = []\n","mape_list = []\n","\n","timestep=12\n","layers=1\n","num_units=128\n","dropout=0.1\n","lr=0.01\n","batch_size=64\n","\n","for z in tqdm(range(10)):\n","  train_X, train_y, valid_X, valid_y, test_X, test_y, scaler, scaler_y = make_data2(hourly, timestep, resid_check=False)\n","  model = Sequential()\n","\n","  if layers > 1:\n","    model.add(GRU(units = num_units, input_shape=(train_X.shape[1], train_X.shape[2]), dropout=dropout, return_sequences=True))\n","\n","    for i in range(layers-2):\n","      model.add(GRU(units = num_units, dropout=dropout, return_sequences=True))\n","\n","    model.add(GRU(units = num_units, dropout=dropout))\n","\n","  else:\n","    model.add(GRU(units = num_units, input_shape=(train_X.shape[1], train_X.shape[2]), dropout=dropout))\n","\n","  model.add(Dense(units = 1))\n","\n","  model.compile(\n","    loss=\"mse\",\n","    optimizer=Adam(learning_rate=lr)\n","  )\n","\n","  model.fit(train_X, train_y, batch_size=batch_size,\n","          epochs=200, verbose=0, shuffle=False,\n","          validation_data=(valid_X, valid_y),\n","          callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n","  \n","  # make a prediction\n","  yhat = model.predict(test_X)\n","  yhat_inv = scaler_y.inverse_transform(yhat)\n","  #resid_sum = (yhat_inv+resid_test.values[3:])\n","\n","  rmse_list.append(sqrt(mean_squared_error(yhat_inv, scaler_y.inverse_transform(test_y))))\n","  mae_list.append(mean_absolute_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","  mape_list.append(mean_absolute_percentage_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","print(f\"GRU LIST OF RMSE: {rmse_list}\")\n","print(f'GRU RMSE:  {sum(rmse_list)/len(rmse_list)}')\n","\n","print(f\"GRU LIST OF MAE: {mae_list}\")\n","print(f'GRU MAE:  {sum(mae_list)/len(mae_list)}')\n","\n","print(f\"GRU LIST OF MAPE: {mape_list}\")\n","print(f'GRU MAPE:  {sum(mape_list)/len(mape_list)}')"]},{"cell_type":"markdown","metadata":{"id":"TE8Ow8HAsDUs"},"source":["# Transformer"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"eLKekJI7sK17","executionInfo":{"status":"ok","timestamp":1649729831253,"user_tz":-540,"elapsed":37,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["from tensorflow.keras import layers\n","\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, feat_dim, num_heads, ff_dim, rate = 0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n","        self.ffn = keras.Sequential( [layers.Dense(ff_dim, activation = \"gelu\"), layers.Dense(feat_dim),] )\n","        self.layernorm1 = layers.BatchNormalization()\n","        self.layernorm2 = layers.BatchNormalization()\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","        self.embed_dim = embed_dim\n","        self.feat_dim = feat_dim\n","        self.num_heads = num_heads\n","        self.ff_dim = ff_dim\n","        self.rate = rate\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training = training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training = training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","    def get_config(self):\n","\n","        config = super().get_config()\n","        config.update({\n","            'embed_dim': self.embed_dim,\n","            'feat_dim': self.feat_dim,\n","            'num_heads': self.num_heads,\n","            'ff_dim': self.ff_dim,\n","            'rate': self.rate,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ZBGJvE12sMkF","executionInfo":{"status":"ok","timestamp":1649729831254,"user_tz":-540,"elapsed":35,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["class Time2Vec(keras.layers.Layer):\n","    def __init__(self, kernel_size = 1):\n","        super(Time2Vec, self).__init__(trainable = True, name = 'Time2VecLayer')\n","        self.k = kernel_size\n","\n","    def build(self, input_shape):\n","        # trend\n","        self.wb = self.add_weight(name = 'wb', shape = (input_shape[1],), initializer = 'uniform', trainable = True)\n","        self.bb = self.add_weight(name = 'bb', shape = (input_shape[1],), initializer = 'uniform', trainable = True)\n","        # periodic\n","        self.wa = self.add_weight(name = 'wa', shape = (1, input_shape[1], self.k), initializer = 'uniform', trainable = True)\n","        self.ba = self.add_weight(name = 'ba', shape = (1, input_shape[1], self.k), initializer = 'uniform', trainable = True)\n","        super(Time2Vec, self).build(input_shape)\n","\n","    def call(self, inputs, **kwargs):\n","        bias = self.wb * inputs + self.bb\n","        dp = K.dot(inputs, self.wa) + self.ba\n","        wgts = K.sin(dp) # or K.cos(.)\n","        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n","        ret = K.reshape(ret, (-1, inputs.shape[1] * (self.k + 1)))\n","        return ret\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[1] * (self.k + 1))\n","\n","    def get_config(self):\n","\n","        config = super().get_config()\n","        config.update({\n","            'kernel_size': self.k,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"0ej51TZCsOCO","executionInfo":{"status":"ok","timestamp":1649729831255,"user_tz":-540,"elapsed":34,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}}},"outputs":[],"source":["EPOCHS = 50\n","N_HEADS = 8\n","N_FOLDS = 10\n","FF_DIM = 256\n","N_BLOCKS = 6\n","EMBED_DIM = 64\n","BATCH_SIZE = 16\n","WINDOW_SIZE = 65\n","DROPUT_RATE = 0.0\n","TIME_2_VEC_DIM = 3\n","TRAIN_MODEL = True\n","SKIP_CONNECTION_STRENGTH = 0.9"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"HbPVz-6SwCUD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649730505550,"user_tz":-540,"elapsed":674328,"user":{"displayName":"Guilherme Afonso Galindo Padilha","userId":"04417560273321122628"}},"outputId":"75843ed8-8458-4856-c31a-a93a07022027"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [11:14<00:00, 67.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Transformer LIST OF RMSE: [0.8699763020214683, 0.8413737355416703, 0.7954657191569297, 1.4603522449311717, 0.8118474614125237, 0.9841744960459489, 0.9366877534015465, 0.9012484157088028, 0.7480001808513076, 1.17491102490481]\n","Transformer RMSE:  0.952403733397618\n","Transformer LIST OF MAE: [0.65538, 0.6505493, 0.6062527, 1.1582828, 0.6160899, 0.7662306, 0.7045543, 0.70393693, 0.5753491, 0.76571196]\n","Transformer MAE:  0.7202337503433227\n","Transformer LIST OF MAPE: [0.21512425, 0.25761938, 0.23177643, 0.32690662, 0.3446518, 0.2430423, 0.22583766, 0.52349263, 0.20998694, 0.9320982]\n","Transformer MAPE:  0.3510536223649979\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rmse_list = []\n","mae_list = []\n","mape_list = []\n","\n","batch_size=32\n","lr=0.001\n","N_HEADS = 8\n","FF_DIM = 256\n","N_BLOCKS = 8\n","EMBED_DIM = 32\n","DROPUT_RATE = 0.3\n","time2vec_dim = 4\n","timestep = 7\n","\n","for z in tqdm(range(10)):\n","  train_X, train_y, valid_X, valid_y, test_X, test_y, scaler, scaler_y = make_data2(hourly, timestep, resid_check=False)\n","\n","\n","  input_shape = train_X.shape[1:]\n","  inp = Input(input_shape)\n","  x = inp\n","\n","  time_embedding = keras.layers.TimeDistributed(Time2Vec(time2vec_dim - 1))(x)\n","  x = Concatenate(axis = -1)([x, time_embedding])\n","  x = layers.LayerNormalization(epsilon = 1e-6)(x)\n","\n","  for k in range(N_BLOCKS):\n","    x_old = x\n","    transformer_block = TransformerBlock(EMBED_DIM, input_shape[-1] + ( input_shape[-1] * time2vec_dim), N_HEADS, FF_DIM, DROPUT_RATE)\n","    x = transformer_block(x)\n","    x = ((1.0 - SKIP_CONNECTION_STRENGTH) * x) + (SKIP_CONNECTION_STRENGTH * x_old)\n","\n","  x = layers.Flatten()(x)\n","\n","  x = layers.Dense(128, activation = \"relu\")(x)\n","  x = layers.Dropout(DROPUT_RATE)(x)\n","  x = Dense(1, activation = 'linear')(x)\n","\n","  out = x\n","  model = Model(inp, out)\n","\n","  model.compile(\n","    loss=\"mse\",\n","    optimizer=Adam(learning_rate=lr)\n","              )\n","\n","  model.fit(train_X, train_y, batch_size=batch_size,\n","          epochs=200, verbose=0, shuffle=False,\n","          validation_data=(valid_X, valid_y),\n","          callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]\n","          )\n","  \n","  # make a prediction\n","  yhat = model.predict(test_X)\n","  yhat_inv = scaler_y.inverse_transform(yhat)\n","  #resid_sum = (yhat_inv+resid_test.values[3:])\n","\n","  rmse_list.append(sqrt(mean_squared_error(yhat_inv, scaler_y.inverse_transform(test_y))))\n","  mae_list.append(mean_absolute_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","  mape_list.append(mean_absolute_percentage_error(yhat_inv, scaler_y.inverse_transform(test_y)))\n","print(f\"Transformer LIST OF RMSE: {rmse_list}\")\n","print(f'Transformer RMSE:  {sum(rmse_list)/len(rmse_list)}')\n","\n","print(f\"Transformer LIST OF MAE: {mae_list}\")\n","print(f'Transformer MAE:  {sum(mae_list)/len(mae_list)}')\n","\n","print(f\"Transformer LIST OF MAPE: {mape_list}\")\n","print(f'Transformer MAPE:  {sum(mape_list)/len(mape_list)}')"]}],"metadata":{"colab":{"name":"recifehora_Simple_Test.ipynb","provenance":[{"file_id":"1UnoUXWjuBlqvUsI-FuM7gDk25m5RjR1V","timestamp":1649221331369}],"authorship_tag":"ABX9TyNAHjdhQZpVjKHR2aXzZssI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}